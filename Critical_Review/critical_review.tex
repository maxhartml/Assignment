\documentclass[11pt]{article}

% -- PAGE & MARGIN SETUP -- %
\usepackage[margin=2cm]{geometry} % 2cm margins as required
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage[scaled=1.0]{helvet} % Sans-serif font
\renewcommand{\familydefault}{\sfdefault} % Sans-serif default

\usepackage[dvipsnames]{xcolor}
\definecolor{MyLightBlue}{RGB}{66, 133, 244} 

\usepackage[colorlinks=true, citecolor=MyLightBlue]{hyperref}

% -- FONT SIZE & SPACING -- %
\setstretch{1.0}           % single spacing

% -- HEADER/FOOTER -- %
\pagestyle{fancy}
\fancyhf{} % clear all header and footer
\fancyhead[L]{\small \textbf{Student ID: 2844622 \quad Module Code: 06 32257}}
\fancyhead[R]{\small \textbf{Critical Review Report}}
\fancyfoot[C]{\thepage}
\setlength{\headheight}{14pt}

% -- TITLE STYLE ADJUSTMENTS -- %
\usepackage{titlesec}
% Reduce sizes of section titles
\titleformat{\section}{\bfseries\normalsize}{\thesection.}{1em}{}
\titlespacing{\section}{0pt}{*1.2}{*0.8}
\titleformat{\subsection}{\bfseries\normalsize}{\thesubsection.}{1em}{}
\titlespacing{\subsection}{0pt}{*1.1}{*0.7}

\begin{document}
\thispagestyle{fancy}

% -- TITLE -- %
\begin{center}
    {\large \textbf{Critical Review:}\\
    \textbf{``Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs''}}
\end{center}

\section*{Summary}

The paper "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs" \cite{Tong2024CVPR} investigates a significant challenge faced by multimodal large language models (MLLMs): they often fail to accurately detect low-level, straightforward visual details.
The authors highlight a vulnerability where the reliance on vision encoders employed in MLLMs and that are pre-trained with contrastive language-image learning (e.g. CLIP \cite{Radford2021ICML}) have a tendency to gloss over small but significiant details and patterns. These include object count, orientation, colour and other such fine-grained features.

In section 3 of "Eyes Wide Shut?", Tong et al. show (Figure 2) how they implement the concept of "Clip-blind pairs", pairs of images that appear similar in CLIP's embedding space yet are distinct in the reference self-supervised model (DINOV2) \cite{Oquab2023DINOv2}.
To address these issues, the authors go on to introduce the Multimodal Visual Patterns (MMVP) benchmark. Designed to test MLLMs on simple questions regarding paired images, their empirical results infer that simply scaling CLIP on larger training sets or model sizes does not inherently resolve these vulnerabilities.
Furthermore, their Table 1 outlines that open-source variants like LLaVA-1.5 \cite{Liu2023Arxiv_LLaVA15}, also built on CLIP, fail to capture vital visual cues even when handling advanced reasioning tasks effectively.

Finally, Tong et al. address these issues by proposing an innovative method "Mixture-of-Features" that integrates a purely self-supervised vision encoder (DINOv2 \cite{Oquab2023DINOv2}) with CLIP's language centric features. They go on to show that an additive fusion approach can sometimes degrade the model's ability, however a more sophisticated and nuanced
interleaving of tokens approach, named "interleaved-MoF" sucessfully preserves high-level textual alignment whilst simultaneously imrpoving fine-grained detail recognition. Overall, the paper highlights a latent vulnerability in MLLMs and concludes by offering practical directions of building more detail-sensitive architectures and systems.

\section*{Relation to Other Work}

In the ever-growing research area of MLLMs, early approaches in visual question answering (VQA) tended to solely focus on datasets like VQAv2 \cite{Goyal2017CVPR} or similarly, Visual Genome \cite{Krishna2017IJCV_VisualGenome}, which implemented multi-step reasoning but more often than not failed to isolate subtle visual discrepancies.
More cutting-edge architectures like Winoground \cite{Thrush2022CVPR_Winoground} and MM-Vet \cite{Yu2023Arxiv_MMVet} emphasised integrated and compositional capabilities, showcasing that tasks requiring precise attribute recognition is not always feasible even in advanced systems.
Li et al. go beyong standard benchmarks in their paper "Evaluating object hallucination in large vision-language models" \cite{Li2023Arxiv_POPE}, by analysing and measuring how often a model percieves objects that are not actually present. This provides an interesting parallel to the "Eyes Wide Shut" demonstration of hallucinated details or missed crucial features.

Self-supervised learning in vision has recently seen rapid advancements spearheaded by architectures like DINOv2 \cite{Oquab2023DINOv2} and Masked Autoencoders (MAE) \cite{He2022CVPR_MAE}. 
Tong et al. goes on to extend this research area by demonstrating how the CLIP architecture, despite its early sucesses in areas like zero-shot classification, crucially misinterprets details when asked relatviely straightforward visual questions.
This further embues how "bridging methods" such as "Interleaved-Mof", could be paramount in improving and further driving the text driven nature of CLIP-based encoders.

\section*{Strengths of the Paper}

Tong et al. provide a systematic and controlled way of investigating detailed visual understanding through creating the MMVP bechmark from "CLIP-blind pairs". Systematic failures in misreading orientation or miscounting objects could be now identified, that would otherwise remain latent in broader tasks \cite{Tong2024CVPR}.
Furthermore, one of the paper's  most compelling strengths is its comprehensive empirical analysis. They test GPT-4V \cite{OpenAI2023GPT4} and other popular open-source models like LLaVA-1.5 \cite{Liu2023Arxiv_LLaVA15} on their benchmark and measuring the performance on human baselines. These results are clearly presented in Section 2.3 and Figure 3, wherein each model's shortfalls are demonstrated by specific question examples.
Moreover, when the models dip below random-guess accuracy on straightforward questions, the direct comparison with human responses highlights the importance of the identified weaknesses.

The innovative Mixture-of-Features approach, perhaps, is the most impactful contribution from the paper. By combining two cutting-edge systems, CLIP \cite{Radford2021ICML} and a self-supervised vision model, DINOv2 \cite{Oquab2023DINOv2}, and by interleaving their features they demonstrated measurable progress in visual grounding whilst keeping instruction-following capacity.
Their results garner momentum for future MLLM optimisation, balancing improvded visual understanding with textual alignment.

\section*{Weaknesses of the Paper}

Despite its rigorous methodology, the paper ultimately leaves a few important gaps. First, while the authors demonstrate that missing crucial visual detail is not solved simply by making CLIP-based models larger in size or trained on enlarged datasets, they do not however thoroughly investigate alternative scaling methods.
For instance, different training objectives or more specific fine-grained tasks might somewhat resolve these blind spots. Moreover, the Mixture-of-Features approach, while innovative and effective on the authors' benchmark, is left untested on interactive scenarios like dialogue-based instruction-following. Naturally, this leaves the reader to question its broader applicability on more complex tasks.

Furthermore, the dataset primarily draws on images from ImageNet and LAION, leaving ambiguity on whether CLIP-blind pairs are created similarly in more specialised domains like medical or satelite imagery. Past research in region-based detection suggests local detail can greatly vary in domains where elements are partially occluded or repetitive \cite{Girshick2014CVPR}.
Investigating these phenomenon in more domain specific contexts could further legitimise the paper's claim that such oversights are universal.

Finally, combining two large encoders (CLIP and DINOv2) can be expensive and there is relatively little discussion on potential mitigations, such as freezing layers or knowledge distilation that may reduce overhead. These issues may limit practical use in a resource-constrained setting and ultimately prohibit the deployment of an otherwise sophisticated approach.

\section*{Potential Advancement and Future Work}

Multiple directions emerge to build upon the findings of ``Eyes Wide Shut.'' One natural step is \emph{extending MMVP} into more specialised or high-stakes areas, such as medical imaging, autonomous driving, or fine-grained species identification. If CLIP-blind pairs appear in such settings, it would underscore an urgent need to re-calibrate or retrain vision encoders for safety-critical tasks.

Further, the authors' ``Interleaved-MoF'' approach could evolve into a \emph{dynamic gating} mechanism, wherein the model learns to emphasise the self-supervised features only when local detail is particularly important. Past frameworks like BLIP-2 have hinted at balancing text alignment with vision modules \cite{Li2023ICML_BLIP2}, and a gating-based approach could unify these insights for robust but flexible fusion.

An equally promising line of inquiry is \emph{diagnosing the precise roots of CLIP's failure}. Perhaps introducing local region objectives---akin to early region-based detection networks \cite{Girshick2014CVPR,Lin2017CVPR_FPN}---within the CLIP pipeline would highlight orientation or counting from the outset. Alternatively, training new vision-language models that explicitly integrate ``pixel-level'' self-supervision with textual alignment might preempt the need for post-hoc mixing. Finally, a deeper interpretability analysis, reminiscent of how large language model representations have been dissected \cite{Devlin2019NAACL_BERT}, could reveal whether any architectural modules within CLIP systematically disregard small objects or positional cues.

In summary, ``Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs'' illuminates a significant gap in current vision-language paradigms: while global semantics are well captured, local details are not. By compiling a benchmark that exposes these blind spots and proposing a straightforward albeit computationally heavier fix, the authors make a compelling case that future MLLMs need more balanced, detail-oriented training. The paper's work thus stands as a timely reminder that continuing to escalate dataset size and model scale will not suffice to overcome fundamental representation issues. Instead, combining purely visual (self-supervised) and language-aligned approaches in a more structured, interpretability-conscious manner appears the most promising route to bridging the gap between robust local perception and fluent linguistic output.

% -- REFERENCES -- %
\begin{small}
\begin{thebibliography}{9}
    \setlength{\itemsep}{0.1em}  
    \setlength{\parskip}{0pt}     

        \bibitem{Tong2024CVPR}
        S.~Tong \emph{et al.},
        ``Eyes wide shut? Exploring the visual shortcomings of multimodal LLMs''
        in \emph{CVPR}, 2024.
        
        \bibitem{Radford2021ICML}
        A.~Radford \emph{et al.},
        ``Learning transferable visual models from natural language supervision''
        in \emph{ICML}, 2021.

        \bibitem{Oquab2023DINOv2}
        M.~Oquab \emph{et al.},
        ``DINOv2: learning robust visual features without supervision''
        \emph{arXiv:2304.07193}, 2023.

        \bibitem{Liu2023Arxiv_LLaVA15}
        H.~Liu \emph{et al.},
        ``Improved baselines with visual instruction tuning,''
        \emph{arXiv:2310.03744}, 2023.

        \bibitem{Goyal2017CVPR}
        Y.~Goyal \emph{et al.},
        ``Making the V in VQA matter,''
        in \emph{CVPR}, 2017.

        \bibitem{Krishna2017IJCV_VisualGenome}
        R.~Krishna \emph{et al.},
        ``Visual genome: connecting language and vision using crowdsourced dense image annotations''
        \emph{IJCV}, 2017.

        \bibitem{Thrush2022CVPR_Winoground}
        T.~Thrush \emph{et al.},
        ``Winoground: probing vision and language models for visio-linguistic compositionality''
        in \emph{CVPR}, 2022.

        \bibitem{Yu2023Arxiv_MMVet}
        W.~Yu \emph{et al.},
        ``MM-Vet: evaluating large multimodal models for integrated capabilities''
        \emph{arXiv:2308.02490}, 2023.

        \bibitem{Li2023Arxiv_POPE}
        Y.~Li \emph{et al.},
        ``Evaluating object hallucination in large vision-language models''
        \emph{arXiv:2305.10355}, 2023.
            
        \bibitem{He2022CVPR_MAE}
        K.~He \emph{et al.},
        ``Masked autoencoders are scalable vision learners,''
        in \emph{CVPR}, 2022.
        
        \bibitem{OpenAI2023GPT4}
        OpenAI,
        ``GPT-4 technical report,''
        Tech. Rep., 2023.

        \bibitem{Girshick2014CVPR}
        R. ~Girshick \emph{et al.},
        ``Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,''
        in \emph{CVPR}, 2014
        
\end{thebibliography}
\end{small}
    
\end{document}